{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGBG6iJU_MxG"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXtw2JOp_QJ5"
      },
      "source": [
        "This notebook implements the \"Deep Q-Network (DQN)\" algorithm for the \"Gridworld\" environment. The code for DQN is fully explained, with comments for each line, in Chapter 3 of the book \"Deep Reinforcement Learning in Action\" by Alexander Zai and Brandon Brown. You can find all the codes in this book in the book's GitHub repository: [DeepReinforcementLearningInAction](https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePriqQru-7mB"
      },
      "source": [
        "# 2. Python Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Import Packages"
      ],
      "metadata": {
        "id": "4w646KrSYMmx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuW3ywki9qOJ"
      },
      "source": [
        "%matplotlib inline\n",
        "import sys\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Create Environment"
      ],
      "metadata": {
        "id": "PLR50Zgwm6vL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1. Create Gridboard for Gridworld Environment"
      ],
      "metadata": {
        "id": "8S2ZLJlnfRJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def randPair(s,e):\n",
        "    return np.random.randint(s,e), np.random.randint(s,e)\n",
        "\n",
        "class BoardPiece:\n",
        "\n",
        "    def __init__(self, name, code, pos):\n",
        "        self.name = name #name of the piece\n",
        "        self.code = code #an ASCII character to display on the board\n",
        "        self.pos = pos #2-tuple e.g. (1,4)\n",
        "\n",
        "class BoardMask:\n",
        "\n",
        "    def __init__(self, name, mask, code):\n",
        "        self.name = name\n",
        "        self.mask = mask\n",
        "        self.code = code\n",
        "\n",
        "    def get_positions(self): #returns tuple of arrays\n",
        "        return np.nonzero(self.mask)\n",
        "\n",
        "def zip_positions2d(positions): #positions is tuple of two arrays\n",
        "    x,y = positions\n",
        "    return list(zip(x,y))\n",
        "\n",
        "class GridBoard:\n",
        "\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size #Board dimensions, e.g. 4 x 4\n",
        "        self.components = {} #name : board piece\n",
        "        self.masks = {}\n",
        "\n",
        "    def addPiece(self, name, code, pos=(0,0)):\n",
        "        newPiece = BoardPiece(name, code, pos)\n",
        "        self.components[name] = newPiece\n",
        "\n",
        "    #basically a set of boundary elements\n",
        "    def addMask(self, name, mask, code):\n",
        "        #mask is a 2D-numpy array with 1s where the boundary elements are\n",
        "        newMask = BoardMask(name, mask, code)\n",
        "        self.masks[name] = newMask\n",
        "\n",
        "    def movePiece(self, name, pos):\n",
        "        move = True\n",
        "        for _, mask in self.masks.items():\n",
        "            if pos in zip_positions2d(mask.get_positions()):\n",
        "                move = False\n",
        "        if move:\n",
        "            self.components[name].pos = pos\n",
        "\n",
        "    def delPiece(self, name):\n",
        "        del self.components['name']\n",
        "\n",
        "    def render(self):\n",
        "        dtype = '<U2'\n",
        "        displ_board = np.zeros((self.size, self.size), dtype=dtype)\n",
        "        displ_board[:] = ' '\n",
        "\n",
        "        for name, piece in self.components.items():\n",
        "            displ_board[piece.pos] = piece.code\n",
        "\n",
        "        for name, mask in self.masks.items():\n",
        "            displ_board[mask.get_positions()] = mask.code\n",
        "\n",
        "        return displ_board\n",
        "\n",
        "    def render_np(self):\n",
        "        num_pieces = len(self.components) + len(self.masks)\n",
        "        displ_board = np.zeros((num_pieces, self.size, self.size), dtype=np.uint8)\n",
        "        layer = 0\n",
        "        for name, piece in self.components.items():\n",
        "            pos = (layer,) + piece.pos\n",
        "            displ_board[pos] = 1\n",
        "            layer += 1\n",
        "\n",
        "        for name, mask in self.masks.items():\n",
        "            x,y = self.masks['boundary'].get_positions()\n",
        "            z = np.repeat(layer,len(x))\n",
        "            a = (z,x,y)\n",
        "            displ_board[a] = 1\n",
        "            layer += 1\n",
        "        return displ_board\n",
        "\n",
        "def addTuple(a,b):\n",
        "    return tuple([sum(x) for x in zip(a,b)])"
      ],
      "metadata": {
        "id": "YdKeoRDSm_nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2. Create Gridworld Environment"
      ],
      "metadata": {
        "id": "q00qRXREfl1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gridworld:\n",
        "\n",
        "    def __init__(self, size=4, mode='static'):\n",
        "        if size >= 4:\n",
        "            self.board = GridBoard(size=size)\n",
        "        else:\n",
        "            print(\"Minimum board size is 4. Initialized to size 4.\")\n",
        "            self.board = GridBoard(size=4)\n",
        "\n",
        "        #Add pieces, positions will be updated later\n",
        "        self.board.addPiece('Player','P',(0,0))\n",
        "        self.board.addPiece('Goal','+',(1,0))\n",
        "        self.board.addPiece('Pit','-',(2,0))\n",
        "        self.board.addPiece('Wall','W',(3,0))\n",
        "\n",
        "        if mode == 'static':\n",
        "            self.initGridStatic()\n",
        "        elif mode == 'player':\n",
        "            self.initGridPlayer()\n",
        "        else:\n",
        "            self.initGridRand()\n",
        "\n",
        "    #Initialize stationary grid, all items are placed deterministically\n",
        "    def initGridStatic(self):\n",
        "        #Setup static pieces\n",
        "        self.board.components['Player'].pos = (0,3) #Row, Column\n",
        "        self.board.components['Goal'].pos = (0,0)\n",
        "        self.board.components['Pit'].pos = (0,1)\n",
        "        self.board.components['Wall'].pos = (1,1)\n",
        "\n",
        "    #Check if board is initialized appropriately (no overlapping pieces)\n",
        "    #also remove impossible-to-win boards\n",
        "    def validateBoard(self):\n",
        "        valid = True\n",
        "\n",
        "        player = self.board.components['Player']\n",
        "        goal = self.board.components['Goal']\n",
        "        wall = self.board.components['Wall']\n",
        "        pit = self.board.components['Pit']\n",
        "\n",
        "        all_positions = [piece for name,piece in self.board.components.items()]\n",
        "        all_positions = [player.pos, goal.pos, wall.pos, pit.pos]\n",
        "        if len(all_positions) > len(set(all_positions)):\n",
        "            return False\n",
        "\n",
        "        corners = [(0,0),(0,self.board.size), (self.board.size,0), (self.board.size,self.board.size)]\n",
        "        #if player is in corner, can it move? if goal is in corner, is it blocked?\n",
        "        if player.pos in corners or goal.pos in corners:\n",
        "            val_move_pl = [self.validateMove('Player', addpos) for addpos in [(0,1),(1,0),(-1,0),(0,-1)]]\n",
        "            val_move_go = [self.validateMove('Goal', addpos) for addpos in [(0,1),(1,0),(-1,0),(0,-1)]]\n",
        "            if 0 not in val_move_pl or 0 not in val_move_go:\n",
        "                #print(self.display())\n",
        "                #print(\"Invalid board. Re-initializing...\")\n",
        "                valid = False\n",
        "\n",
        "        return valid\n",
        "\n",
        "    #Initialize player in random location, but keep wall, goal and pit stationary\n",
        "    def initGridPlayer(self):\n",
        "        #height x width x depth (number of pieces)\n",
        "        self.initGridStatic()\n",
        "        #place player\n",
        "        self.board.components['Player'].pos = randPair(0,self.board.size)\n",
        "\n",
        "        if (not self.validateBoard()):\n",
        "            #print('Invalid grid. Rebuilding..')\n",
        "            self.initGridPlayer()\n",
        "\n",
        "    #Initialize grid so that goal, pit, wall, player are all randomly placed\n",
        "    def initGridRand(self):\n",
        "        #height x width x depth (number of pieces)\n",
        "        self.board.components['Player'].pos = randPair(0,self.board.size)\n",
        "        self.board.components['Goal'].pos = randPair(0,self.board.size)\n",
        "        self.board.components['Pit'].pos = randPair(0,self.board.size)\n",
        "        self.board.components['Wall'].pos = randPair(0,self.board.size)\n",
        "\n",
        "        if (not self.validateBoard()):\n",
        "            #print('Invalid grid. Rebuilding..')\n",
        "            self.initGridRand()\n",
        "\n",
        "    def validateMove(self, piece, addpos=(0,0)):\n",
        "        outcome = 0 #0 is valid, 1 invalid, 2 lost game\n",
        "        pit = self.board.components['Pit'].pos\n",
        "        wall = self.board.components['Wall'].pos\n",
        "        new_pos = addTuple(self.board.components[piece].pos, addpos)\n",
        "        if new_pos == wall:\n",
        "            outcome = 1 #block move, player can't move to wall\n",
        "        elif max(new_pos) > (self.board.size-1):    #if outside bounds of board\n",
        "            outcome = 1\n",
        "        elif min(new_pos) < 0: #if outside bounds\n",
        "            outcome = 1\n",
        "        elif new_pos == pit:\n",
        "            outcome = 2\n",
        "\n",
        "        return outcome\n",
        "\n",
        "    def makeMove(self, action):\n",
        "        #need to determine what object (if any) is in the new grid spot the player is moving to\n",
        "        #actions in {u,d,l,r}\n",
        "        def checkMove(addpos):\n",
        "            if self.validateMove('Player', addpos) in [0,2]:\n",
        "                new_pos = addTuple(self.board.components['Player'].pos, addpos)\n",
        "                self.board.movePiece('Player', new_pos)\n",
        "\n",
        "        if action == 'u': #up\n",
        "            checkMove((-1,0))\n",
        "        elif action == 'd': #down\n",
        "            checkMove((1,0))\n",
        "        elif action == 'l': #left\n",
        "            checkMove((0,-1))\n",
        "        elif action == 'r': #right\n",
        "            checkMove((0,1))\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def reward(self):\n",
        "        if (self.board.components['Player'].pos == self.board.components['Pit'].pos):\n",
        "            return -10\n",
        "        elif (self.board.components['Player'].pos == self.board.components['Goal'].pos):\n",
        "            return 10\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def display(self):\n",
        "        return self.board.render()"
      ],
      "metadata": {
        "id": "lrXHuYHJflTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Create Q-Function (DQN Agent)"
      ],
      "metadata": {
        "id": "OopbrglvX9EK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1. Define Hyperparameters"
      ],
      "metadata": {
        "id": "RSexXXlzlNxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = 64\n",
        "l2 = 150\n",
        "l3 = 100\n",
        "l4 = 4\n",
        "\n",
        "sync_freq = 50  # Synchronizes the frequency parameter; every 50 steps we will copy the parameters of model into model2\n",
        "learning_rate = 1e-3\n",
        "\n",
        "gamma = 0.9\n",
        "epsilon = 1.0\n",
        "\n",
        "action_set = {\n",
        "    0: \"u\",\n",
        "    1: \"d\",\n",
        "    2: \"l\",\n",
        "    3: \"r\"\n",
        "}\n",
        "\n",
        "epochs = 5000\n",
        "losses = []  # Creates a list to store loss values so we can plot the trend later\n",
        "\n",
        "mem_size = 1000  # Sets the total size of the experience relay memory\n",
        "batch_size = 200  # Sets the mini-batch size\n",
        "replay = deque(maxlen=mem_size)  # Creates a memory replay as a deque list\n",
        "max_moves = 50  # Sets the maximum number of moves before the game is over\n",
        "h = 0\n",
        "sync_freq = 500  # Sets the update frequency for synchronizing the target model parameters to the main DQN\n",
        "j = 0"
      ],
      "metadata": {
        "id": "Z3S0aNIRmu2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2. Q-Function"
      ],
      "metadata": {
        "id": "CU-ADRpulT3j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZuOtaHY9qT5"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(l1, l2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(l2, l3),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(l3, l4)\n",
        ").to(device)\n",
        "\n",
        "model2 = model2 = copy.deepcopy(model)  # Creates a second model by making an identical copy of the original Q-network model\n",
        "model2.load_state_dict(model.state_dict())  # Copies the parameters of the original model\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3. Train Model"
      ],
      "metadata": {
        "id": "8a_564VyYIiJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiaPvTc79qhf"
      },
      "source": [
        "for i in range(epochs):  # The main training loop\n",
        "\n",
        "    game = Gridworld(size=4, mode=\"random\")  # For each epoch, we start a new game.\n",
        "    state1_ = game.board.render_np().reshape(1, 64) + np.random.rand(1, 64) / 100.0  # After we create the game, we extract the state information and add a small amount of noise.\n",
        "    state1 = torch.from_numpy(state1_).float().to(device)  # Converts the numpy array into a PyTorch tensor and then into a PyTorch variable\n",
        "    status = 1  # Uses the status variable to keep track of whether or not the game is still in progress.\n",
        "    mov = 0\n",
        "\n",
        "    while(status == 1):  # While this game is still in progress, plays to completion and then starts a new epoch\n",
        "\n",
        "        j += 1\n",
        "        mov += 1\n",
        "        qval = model(state1).to(\"cpu\")  # Runs the Q-network forward to get its predicted Q values for all the actions\n",
        "        qval_ = qval.data.numpy()\n",
        "\n",
        "        ##########################################\n",
        "        # Epsilon-Greedy Action Selection Strategy\n",
        "        if (random.random() < epsilon):\n",
        "            action_ = np.random.randint(0, 4)\n",
        "        else:\n",
        "            action_ = np.argmax(qval_)\n",
        "        ##########################################\n",
        "\n",
        "        action = action_set[action_]  # Translates the numerical action into one of the action characters that our Gridworld game expects\n",
        "        game.makeMove(action)  # After selecting an action, takes the action\n",
        "        state2_ = game.board.render_np().reshape(1, 64) + np.random.rand(1, 64) / 100.0\n",
        "        state2 = torch.from_numpy(state2_).float().to(device)  # After making a move, gets the new state of the game\n",
        "        reward = game.reward()\n",
        "        done = True if reward > 0 else False\n",
        "        exp = (state1, action_, reward, state2, done)  # Creates an experience of state, reward, action, and the next state as tuple\n",
        "        replay.append(exp)  # Adds the experience to the experience replay list\n",
        "        state1 = state2\n",
        "\n",
        "        if len(replay) > batch_size:  # If the replay list isat least as long as the mini-batch size, begins the mini-batch training\n",
        "\n",
        "            minibatch = random.sample(replay, batch_size)  # Randomly samples a subset of the replay list\n",
        "\n",
        "            ##################################################################################\n",
        "            # Separates out the components of each experience into separate mini-batch tensors\n",
        "            state1_batch = torch.cat([s1 for (s1, a, r, s2, d) in minibatch]).to(device)\n",
        "            action_batch = torch.Tensor([a for (s1, a, r, s2, d) in minibatch]).to(device)\n",
        "            reward_batch = torch.Tensor([r for (s1, a, r, s2, d) in minibatch]).to(device)\n",
        "            state2_batch = torch.cat([s2 for (s1, a, r, s2, d) in minibatch]).to(device)\n",
        "            done_batch = torch.Tensor([d for (s1, a, r, s2, d) in minibatch]).to(device)\n",
        "            ##################################################################################\n",
        "\n",
        "            Q1 = model(state1_batch)  # Recomputes Q values for the mini-batch os states to get gradients\n",
        "            with torch.no_grad():  # By using the torch.no_grad() context, we tell PyTorch to not create a computational graph for the code within the context; this will save memory when we don’t need the computational graph.\n",
        "                Q2 = model2(state2_batch)  # Uses the target network to get the maximum Q value for the next state\n",
        "\n",
        "            Y = reward_batch + gamma * ((1 - done_batch) * torch.max(Q2, dim=1)[0])  # Computes the target Q values we want the DQN to learn\n",
        "            X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
        "            loss = loss_fn(X, Y.detach())\n",
        "\n",
        "#             print(i, loss.item())\n",
        "#             display.clear_output(wait=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "            optimizer.step()\n",
        "\n",
        "            if j % sync_freq == 0:  # Copies the main model parameters to the target network\n",
        "                model2.load_state_dict(model.state_dict())\n",
        "\n",
        "        if reward != -1 or mov > max_moves:  # If reward is -1, the game has not been won or lost and is still in progress\n",
        "            status = 0\n",
        "            mov = 0\n",
        "\n",
        "    if epsilon > 0.1:  # Decrements the epsilon value each epoch\n",
        "        epsilon -= (1 / epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.4. Plot"
      ],
      "metadata": {
        "id": "m_hAJwwwhspY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "font1 = {\"family\": \"serif\",\n",
        "         \"color\":  \"black\",\n",
        "         \"weight\": \"normal\",\n",
        "         \"size\": 14}\n",
        "\n",
        "font2 = {\"family\": \"serif\",\n",
        "         \"color\":  \"black\",\n",
        "         \"weight\": \"bold\",\n",
        "         \"size\": 14}\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6), dpi=300)\n",
        "ax = plt.subplot(111)\n",
        "\n",
        "ax.set_xlabel(\"Epochs\", fontdict=font1)\n",
        "ax.set_ylabel(\"Loss\", fontdict=font1)\n",
        "ax.set_title(\"Title\", fontdict=font2)\n",
        "\n",
        "ax.tick_params(axis=\"both\", which=\"both\", direction=\"inout\", labelsize=\"large\", grid_alpha=0.4)\n",
        "\n",
        "for tick in ax.get_xticklabels():\n",
        "    tick.set_fontname(\"serif\")\n",
        "for tick in ax.get_yticklabels():\n",
        "    tick.set_fontname(\"serif\")\n",
        "\n",
        "# set the limits\n",
        "# ax.set_xlim(0, 24)\n",
        "# ax.set_ylim(6, 24)\n",
        "\n",
        "# set the grid on\n",
        "ax.grid(\"on\")\n",
        "\n",
        "# plt.plot(np.linspace(0, 999, 1000), np.array(losses), \"g\")\n",
        "ax.plot(np.linspace(0, len(losses) - 1, len(losses)), np.array(losses), \"g\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hZzdZySFhwha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.5. Test Model"
      ],
      "metadata": {
        "id": "3Cv1mBRJhxAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, mode=\"static\", display=True):\n",
        "    i = 0\n",
        "    test_game = Gridworld(mode=mode)\n",
        "    state_ = test_game.board.render_np().reshape(1, 64) + np.random.rand(1, 64) / 10.0\n",
        "    state = torch.from_numpy(state_).float().to(device)\n",
        "    if display:\n",
        "        print(\"Initial State:\")\n",
        "        print(test_game.display())\n",
        "    status = 1\n",
        "    while (status == 1):\n",
        "        qval = model(state).to(\"cpu\")\n",
        "        qval_ = qval.data.numpy()\n",
        "        action_ = np.argmax(qval_)  # Takes the action with the highest Q value\n",
        "        action = action_set[action_]\n",
        "        if display:\n",
        "            print(\"Move #: %s; Taking action: %s\" % (i, action))\n",
        "        test_game.makeMove(action)\n",
        "        state_ = test_game.board.render_np().reshape(1, 64) + np.random.rand(1, 64) / 10.0\n",
        "        state = torch.from_numpy(state_).float().to(device)\n",
        "        if display:\n",
        "            print(test_game.display())\n",
        "        reward = test_game.reward()\n",
        "        if reward != -1:\n",
        "            if reward > 0:\n",
        "                status = 2\n",
        "                if display:\n",
        "                    print(\"Game won! Reward: %s\" % (reward,))\n",
        "            else:\n",
        "                status = 0\n",
        "                if display:\n",
        "                    print(\"Game won! Reward: %s\" % (reward,))\n",
        "        i += 1\n",
        "        if (i > 15):\n",
        "            if display:\n",
        "                print(\"Game lost; too many moves.\")\n",
        "            break\n",
        "    win = True if status == 2 else False\n",
        "    return win\n",
        "\n",
        "test_model(model, 'static')"
      ],
      "metadata": {
        "id": "fMZOpqYLhxYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}